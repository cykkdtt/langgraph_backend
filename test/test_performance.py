#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿæ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•å“åº”æ—¶é—´ã€å¹¶å‘æ€§èƒ½å’Œèµ„æºä½¿ç”¨æƒ…å†µ
"""

import requests
import time
import threading
import json
import statistics
import psutil
import concurrent.futures
from typing import List, Dict, Any
from datetime import datetime

class PerformanceTester:
    def __init__(self, backend_url="http://localhost:8000", frontend_url="http://localhost:5173"):
        self.backend_url = backend_url
        self.frontend_url = frontend_url
        self.test_results = []
        self.session = requests.Session()
        
    def log_test(self, test_name: str, metrics: Dict[str, Any]):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            "test_name": test_name,
            "metrics": metrics,
            "timestamp": datetime.now().isoformat()
        }
        self.test_results.append(result)
        print(f"ğŸ“Š {test_name}:")
        for key, value in metrics.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
        print()
        
    def measure_response_time(self, url: str, method: str = "GET", data: Dict = None, iterations: int = 10) -> Dict[str, float]:
        """æµ‹é‡å“åº”æ—¶é—´"""
        response_times = []
        successful_requests = 0
        
        for _ in range(iterations):
            try:
                start_time = time.time()
                
                if method == "GET":
                    response = self.session.get(url, timeout=10)
                elif method == "POST":
                    response = self.session.post(url, json=data, timeout=10)
                    
                end_time = time.time()
                response_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                
                if response.status_code in [200, 201]:
                    response_times.append(response_time)
                    successful_requests += 1
                    
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥: {e}")
                
        if response_times:
            return {
                "å¹³å‡å“åº”æ—¶é—´(ms)": statistics.mean(response_times),
                "æœ€å°å“åº”æ—¶é—´(ms)": min(response_times),
                "æœ€å¤§å“åº”æ—¶é—´(ms)": max(response_times),
                "ä¸­ä½æ•°å“åº”æ—¶é—´(ms)": statistics.median(response_times),
                "æˆåŠŸè¯·æ±‚æ•°": successful_requests,
                "æ€»è¯·æ±‚æ•°": iterations,
                "æˆåŠŸç‡(%)": (successful_requests / iterations) * 100
            }
        else:
            return {
                "å¹³å‡å“åº”æ—¶é—´(ms)": 0,
                "æœ€å°å“åº”æ—¶é—´(ms)": 0,
                "æœ€å¤§å“åº”æ—¶é—´(ms)": 0,
                "ä¸­ä½æ•°å“åº”æ—¶é—´(ms)": 0,
                "æˆåŠŸè¯·æ±‚æ•°": 0,
                "æ€»è¯·æ±‚æ•°": iterations,
                "æˆåŠŸç‡(%)": 0
            }
            
    def test_concurrent_requests(self, url: str, concurrent_users: int = 10, requests_per_user: int = 5) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
        def make_request():
            try:
                start_time = time.time()
                response = self.session.get(url, timeout=10)
                end_time = time.time()
                return {
                    "response_time": (end_time - start_time) * 1000,
                    "status_code": response.status_code,
                    "success": response.status_code == 200
                }
            except Exception as e:
                return {
                    "response_time": 0,
                    "status_code": 0,
                    "success": False,
                    "error": str(e)
                }
                
        # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘è¯·æ±‚
        all_results = []
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = []
            for _ in range(concurrent_users * requests_per_user):
                future = executor.submit(make_request)
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                all_results.append(result)
                
        end_time = time.time()
        total_time = end_time - start_time
        
        # åˆ†æç»“æœ
        successful_requests = [r for r in all_results if r['success']]
        response_times = [r['response_time'] for r in successful_requests]
        
        if response_times:
            return {
                "å¹¶å‘ç”¨æˆ·æ•°": concurrent_users,
                "æ¯ç”¨æˆ·è¯·æ±‚æ•°": requests_per_user,
                "æ€»è¯·æ±‚æ•°": len(all_results),
                "æˆåŠŸè¯·æ±‚æ•°": len(successful_requests),
                "å¤±è´¥è¯·æ±‚æ•°": len(all_results) - len(successful_requests),
                "æˆåŠŸç‡(%)": (len(successful_requests) / len(all_results)) * 100,
                "æ€»è€—æ—¶(s)": total_time,
                "å¹³å‡å“åº”æ—¶é—´(ms)": statistics.mean(response_times),
                "æœ€å¤§å“åº”æ—¶é—´(ms)": max(response_times),
                "æœ€å°å“åº”æ—¶é—´(ms)": min(response_times),
                "ååé‡(req/s)": len(successful_requests) / total_time if total_time > 0 else 0
            }
        else:
            return {
                "å¹¶å‘ç”¨æˆ·æ•°": concurrent_users,
                "æ¯ç”¨æˆ·è¯·æ±‚æ•°": requests_per_user,
                "æ€»è¯·æ±‚æ•°": len(all_results),
                "æˆåŠŸè¯·æ±‚æ•°": 0,
                "å¤±è´¥è¯·æ±‚æ•°": len(all_results),
                "æˆåŠŸç‡(%)": 0,
                "æ€»è€—æ—¶(s)": total_time,
                "å¹³å‡å“åº”æ—¶é—´(ms)": 0,
                "æœ€å¤§å“åº”æ—¶é—´(ms)": 0,
                "æœ€å°å“åº”æ—¶é—´(ms)": 0,
                "ååé‡(req/s)": 0
            }
            
    def get_system_metrics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            return {
                "CPUä½¿ç”¨ç‡(%)": cpu_percent,
                "å†…å­˜ä½¿ç”¨ç‡(%)": memory.percent,
                "å¯ç”¨å†…å­˜(GB)": memory.available / (1024**3),
                "æ€»å†…å­˜(GB)": memory.total / (1024**3),
                "ç£ç›˜ä½¿ç”¨ç‡(%)": disk.percent,
                "å¯ç”¨ç£ç›˜ç©ºé—´(GB)": disk.free / (1024**3)
            }
        except Exception as e:
            return {"é”™è¯¯": f"æ— æ³•è·å–ç³»ç»ŸæŒ‡æ ‡: {str(e)}"}
            
    def test_endpoint_performance(self):
        """æµ‹è¯•å„ä¸ªç«¯ç‚¹çš„æ€§èƒ½"""
        endpoints = [
            (f"{self.backend_url}/health", "GET", None, "åç«¯å¥åº·æ£€æŸ¥"),
            (f"{self.backend_url}/", "GET", None, "åç«¯æ ¹è·¯å¾„"),
            (f"{self.backend_url}/agents", "GET", None, "æ™ºèƒ½ä½“åˆ—è¡¨"),
            (f"{self.frontend_url}", "GET", None, "å‰ç«¯é¦–é¡µ")
        ]
        
        for url, method, data, description in endpoints:
            print(f"ğŸ” æµ‹è¯• {description} æ€§èƒ½...")
            metrics = self.measure_response_time(url, method, data, iterations=20)
            self.log_test(f"{description} - å“åº”æ—¶é—´æµ‹è¯•", metrics)
            
    def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        test_cases = [
            (f"{self.backend_url}/health", 5, 3, "åç«¯å¥åº·æ£€æŸ¥ - è½»è´Ÿè½½"),
            (f"{self.backend_url}/health", 10, 5, "åç«¯å¥åº·æ£€æŸ¥ - ä¸­è´Ÿè½½"),
            (f"{self.backend_url}/agents", 5, 3, "æ™ºèƒ½ä½“åˆ—è¡¨ - è½»è´Ÿè½½"),
            (f"{self.frontend_url}", 5, 3, "å‰ç«¯é¦–é¡µ - è½»è´Ÿè½½")
        ]
        
        for url, concurrent_users, requests_per_user, description in test_cases:
            print(f"ğŸš€ æµ‹è¯• {description} å¹¶å‘æ€§èƒ½...")
            metrics = self.test_concurrent_requests(url, concurrent_users, requests_per_user)
            self.log_test(f"{description} - å¹¶å‘æµ‹è¯•", metrics)
            
    def test_load_performance(self):
        """æµ‹è¯•è´Ÿè½½æ€§èƒ½"""
        print("âš¡ æµ‹è¯•ç³»ç»Ÿè´Ÿè½½æ€§èƒ½...")
        
        # æµ‹è¯•ä¸åŒè´Ÿè½½çº§åˆ«
        load_tests = [
            (f"{self.backend_url}/health", 20, 10, "é«˜è´Ÿè½½æµ‹è¯•"),
            (f"{self.backend_url}/agents", 15, 8, "ä¸­é«˜è´Ÿè½½æµ‹è¯•")
        ]
        
        for url, concurrent_users, requests_per_user, description in load_tests:
            print(f"ğŸ“ˆ æ‰§è¡Œ {description}...")
            
            # æµ‹è¯•å‰è·å–ç³»ç»ŸæŒ‡æ ‡
            before_metrics = self.get_system_metrics()
            
            # æ‰§è¡Œè´Ÿè½½æµ‹è¯•
            performance_metrics = self.test_concurrent_requests(url, concurrent_users, requests_per_user)
            
            # æµ‹è¯•åè·å–ç³»ç»ŸæŒ‡æ ‡
            time.sleep(2)  # ç­‰å¾…ç³»ç»Ÿç¨³å®š
            after_metrics = self.get_system_metrics()
            
            # åˆå¹¶æŒ‡æ ‡
            combined_metrics = {
                **performance_metrics,
                "æµ‹è¯•å‰CPUä½¿ç”¨ç‡(%)": before_metrics.get("CPUä½¿ç”¨ç‡(%)", 0),
                "æµ‹è¯•åCPUä½¿ç”¨ç‡(%)": after_metrics.get("CPUä½¿ç”¨ç‡(%)", 0),
                "æµ‹è¯•å‰å†…å­˜ä½¿ç”¨ç‡(%)": before_metrics.get("å†…å­˜ä½¿ç”¨ç‡(%)", 0),
                "æµ‹è¯•åå†…å­˜ä½¿ç”¨ç‡(%)": after_metrics.get("å†…å­˜ä½¿ç”¨ç‡(%)", 0)
            }
            
            self.log_test(f"{description}", combined_metrics)
            
    def run_all_performance_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("\nğŸš€ å¼€å§‹ç³»ç»Ÿæ€§èƒ½æµ‹è¯•...\n")
        
        # è·å–åˆå§‹ç³»ç»ŸçŠ¶æ€
        print("ğŸ“Š ç³»ç»Ÿåˆå§‹çŠ¶æ€:")
        initial_metrics = self.get_system_metrics()
        self.log_test("ç³»ç»Ÿåˆå§‹çŠ¶æ€", initial_metrics)
        
        # ç«¯ç‚¹å“åº”æ—¶é—´æµ‹è¯•
        print("â±ï¸ ç«¯ç‚¹å“åº”æ—¶é—´æµ‹è¯•:")
        self.test_endpoint_performance()
        
        # å¹¶å‘æ€§èƒ½æµ‹è¯•
        print("ğŸ”„ å¹¶å‘æ€§èƒ½æµ‹è¯•:")
        self.test_concurrent_performance()
        
        # è´Ÿè½½æ€§èƒ½æµ‹è¯•
        print("ğŸ“ˆ è´Ÿè½½æ€§èƒ½æµ‹è¯•:")
        self.test_load_performance()
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        self.generate_performance_report()
        
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        # åˆ†æå“åº”æ—¶é—´
        response_time_tests = [r for r in self.test_results if "å“åº”æ—¶é—´æµ‹è¯•" in r['test_name']]
        if response_time_tests:
            avg_response_times = [r['metrics'].get('å¹³å‡å“åº”æ—¶é—´(ms)', 0) for r in response_time_tests]
            print(f"\nâ±ï¸ å“åº”æ—¶é—´åˆ†æ:")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {statistics.mean(avg_response_times):.2f}ms")
            print(f"  æœ€å¿«å“åº”æ—¶é—´: {min(avg_response_times):.2f}ms")
            print(f"  æœ€æ…¢å“åº”æ—¶é—´: {max(avg_response_times):.2f}ms")
        
        # åˆ†æå¹¶å‘æ€§èƒ½
        concurrent_tests = [r for r in self.test_results if "å¹¶å‘æµ‹è¯•" in r['test_name']]
        if concurrent_tests:
            throughputs = [r['metrics'].get('ååé‡(req/s)', 0) for r in concurrent_tests]
            success_rates = [r['metrics'].get('æˆåŠŸç‡(%)', 0) for r in concurrent_tests]
            print(f"\nğŸš€ å¹¶å‘æ€§èƒ½åˆ†æ:")
            print(f"  å¹³å‡ååé‡: {statistics.mean(throughputs):.2f} req/s")
            print(f"  æœ€é«˜ååé‡: {max(throughputs):.2f} req/s")
            print(f"  å¹³å‡æˆåŠŸç‡: {statistics.mean(success_rates):.1f}%")
        
        # æ€§èƒ½è¯„çº§
        print(f"\nğŸ† æ€§èƒ½è¯„çº§:")
        if response_time_tests:
            avg_response = statistics.mean([r['metrics'].get('å¹³å‡å“åº”æ—¶é—´(ms)', 0) for r in response_time_tests])
            if avg_response < 100:
                print("  å“åº”æ—¶é—´: ä¼˜ç§€ (< 100ms)")
            elif avg_response < 500:
                print("  å“åº”æ—¶é—´: è‰¯å¥½ (< 500ms)")
            elif avg_response < 1000:
                print("  å“åº”æ—¶é—´: ä¸€èˆ¬ (< 1000ms)")
            else:
                print("  å“åº”æ—¶é—´: éœ€è¦ä¼˜åŒ– (> 1000ms)")
        
        if concurrent_tests:
            avg_success_rate = statistics.mean([r['metrics'].get('æˆåŠŸç‡(%)', 0) for r in concurrent_tests])
            if avg_success_rate >= 95:
                print("  ç¨³å®šæ€§: ä¼˜ç§€ (â‰¥ 95%)")
            elif avg_success_rate >= 90:
                print("  ç¨³å®šæ€§: è‰¯å¥½ (â‰¥ 90%)")
            elif avg_success_rate >= 80:
                print("  ç¨³å®šæ€§: ä¸€èˆ¬ (â‰¥ 80%)")
            else:
                print("  ç¨³å®šæ€§: éœ€è¦ä¼˜åŒ– (< 80%)")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = "performance_test_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                "summary": {
                    "total_tests": len(self.test_results),
                    "test_time": datetime.now().isoformat()
                },
                "results": self.test_results
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
if __name__ == "__main__":
    tester = PerformanceTester()
    tester.run_all_performance_tests()