#!/usr/bin/env python3
"""
Modelsç›®å½•ä½¿ç”¨æ¼”ç¤º

æœ¬è„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ /models ç›®å½•ä¸­å®šä¹‰çš„å„ç§æ•°æ®æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºç¡€å“åº”æ¨¡å‹çš„ä½¿ç”¨
2. èŠå¤©ç›¸å…³æ¨¡å‹çš„ä½¿ç”¨
3. æ™ºèƒ½ä½“æ¨¡å‹çš„ä½¿ç”¨
4. è®°å¿†ç®¡ç†æ¨¡å‹çš„ä½¿ç”¨
5. RAGç³»ç»Ÿæ¨¡å‹çš„ä½¿ç”¨
6. æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–
7. APIæ¥å£ä¸­çš„å®é™…åº”ç”¨
"""

import sys
import os
import json
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥å„ç§æ•°æ®æ¨¡å‹
from models.base_models import (
    BaseResponse, SuccessResponse, ErrorResponse,
    PaginatedResponse, HealthCheckResponse, ValidationError
)
from models.chat_models import (
    ChatRequest, ChatResponse, Message, MessageRole, MessageType,
    StreamChunk, ThreadInfo, ToolCall, ToolResult
)
from models.agent_models import (
    AgentInfo, CreateAgentRequest, AgentInstanceRequest, AgentInstanceResponse,
    AgentType, AgentStatus, AgentConfig, AgentTool, AgentCapability
)
from models.memory_models import (
    MemoryItem, MemoryCreateRequest, MemorySearchRequest, MemorySearchResponse,
    MemoryType, MemoryImportance, MemoryStatus
)
from models.rag_models import (
    DocumentModel, DocumentUploadRequest, VectorSearchRequest, VectorSearchResponse,
    RAGRequest, RAGResponse, DocumentType, DocumentStatus
)


def demo_base_models():
    """æ¼”ç¤ºåŸºç¡€å“åº”æ¨¡å‹çš„ä½¿ç”¨"""
    print("=" * 60)
    print("1. åŸºç¡€å“åº”æ¨¡å‹æ¼”ç¤º")
    print("=" * 60)
    
    # æˆåŠŸå“åº”ç¤ºä¾‹
    success_response = SuccessResponse(
        message="æ“ä½œæˆåŠŸ",
        data={"result": "æ•°æ®å¤„ç†å®Œæˆ", "count": 100},
        request_id="req_123456"
    )
    print("âœ… æˆåŠŸå“åº”:")
    print(json.dumps(success_response.dict(), indent=2, ensure_ascii=False, default=str))
    
    # é”™è¯¯å“åº”ç¤ºä¾‹
    error_response = ErrorResponse(
        message="æ“ä½œå¤±è´¥",
        error="å‚æ•°éªŒè¯é”™è¯¯",
        request_id="req_123457"
    )
    print("\nâŒ é”™è¯¯å“åº”:")
    print(json.dumps(error_response.dict(), indent=2, ensure_ascii=False, default=str))
    
    # åˆ†é¡µå“åº”ç¤ºä¾‹
    from models.base_models import PaginationInfo
    pagination = PaginationInfo(
        page=1,
        page_size=20,
        total=100,
        total_pages=5,
        has_next=True,
        has_prev=False
    )
    
    paginated_response = PaginatedResponse[Dict[str, Any]](
        success=True,
        message="è·å–æ•°æ®æˆåŠŸ",
        items=[{"id": i, "name": f"é¡¹ç›®{i}"} for i in range(1, 21)],
        pagination=pagination
    )
    print("\nğŸ“„ åˆ†é¡µå“åº”:")
    print(json.dumps(paginated_response.dict(), indent=2, ensure_ascii=False, default=str))


def demo_chat_models():
    """æ¼”ç¤ºèŠå¤©ç›¸å…³æ¨¡å‹çš„ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("2. èŠå¤©æ¨¡å‹æ¼”ç¤º")
    print("=" * 60)
    
    # èŠå¤©è¯·æ±‚ç¤ºä¾‹
    chat_request = ChatRequest(
        message="ä½ å¥½ï¼Œè¯·å¸®æˆ‘åˆ†æä¸€ä¸‹å¸‚åœºè¶‹åŠ¿",
        agent_id="agent_001",
        stream=True,
        temperature=0.7,
        max_tokens=1000,
        tools=["web_search", "data_analysis"],
        context={"user_preference": "è¯¦ç»†åˆ†æ", "language": "zh-CN"}
    )
    print("ğŸ’¬ èŠå¤©è¯·æ±‚:")
    print(json.dumps(chat_request.dict(), indent=2, ensure_ascii=False, default=str))
    
    # æ¶ˆæ¯æ¨¡å‹ç¤ºä¾‹
    message = Message(
        id="msg_001",
        role=MessageRole.ASSISTANT,
        content="æ ¹æ®æœ€æ–°çš„å¸‚åœºæ•°æ®åˆ†æï¼Œå½“å‰å¸‚åœºå‘ˆç°ä»¥ä¸‹è¶‹åŠ¿...",
        timestamp=datetime.now(),
        metadata={"confidence": 0.95, "sources": ["market_data", "news_analysis"]}
    )
    print("\nğŸ“ æ¶ˆæ¯æ¨¡å‹:")
    print(json.dumps(message.dict(), indent=2, ensure_ascii=False, default=str))
    
    # å·¥å…·è°ƒç”¨ç¤ºä¾‹
    tool_call = ToolCall(
        id="tool_call_001",
        name="web_search",
        arguments={"query": "2024å¹´å¸‚åœºè¶‹åŠ¿åˆ†æ", "limit": 10}
    )
    
    tool_result = ToolResult(
        tool_call_id="tool_call_001",
        result={"results": ["ç»“æœ1", "ç»“æœ2"], "total": 2}
    )
    print("\nğŸ”§ å·¥å…·è°ƒç”¨:")
    print(json.dumps(tool_call.dict(), indent=2, ensure_ascii=False, default=str))
    print("\nğŸ”§ å·¥å…·ç»“æœ:")
    print(json.dumps(tool_result.dict(), indent=2, ensure_ascii=False, default=str))


def demo_agent_models():
    """æ¼”ç¤ºæ™ºèƒ½ä½“æ¨¡å‹çš„ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("3. æ™ºèƒ½ä½“æ¨¡å‹æ¼”ç¤º")
    print("=" * 60)
    
    # æ™ºèƒ½ä½“å·¥å…·é…ç½®
    agent_tools = [
        AgentTool(
            name="web_search",
            description="ç½‘ç»œæœç´¢å·¥å…·",
            enabled=True,
            config={"api_key": "***", "max_results": 10}
        ),
        AgentTool(
            name="data_analysis",
            description="æ•°æ®åˆ†æå·¥å…·",
            enabled=True,
            config={"precision": "high"}
        )
    ]
    
    # æ™ºèƒ½ä½“èƒ½åŠ›é…ç½®
    agent_capabilities = [
        AgentCapability(
            name="natural_language_processing",
            description="è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›",
            enabled=True,
            config={"model": "gpt-4", "language": "zh-CN"}
        ),
        AgentCapability(
            name="data_visualization",
            description="æ•°æ®å¯è§†åŒ–èƒ½åŠ›",
            enabled=True
        )
    ]
    
    # æ™ºèƒ½ä½“é…ç½®
    agent_config = AgentConfig(
        model="gpt-4-turbo",
        temperature=0.7,
        max_tokens=2000,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¸‚åœºåˆ†æå¸ˆï¼Œæ“…é•¿æ•°æ®åˆ†æå’Œè¶‹åŠ¿é¢„æµ‹ã€‚",
        tools=agent_tools,
        capabilities=agent_capabilities,
        memory_config={"type": "long_term", "capacity": 10000},
        rag_config={"enabled": True, "collection": "market_data"}
    )
    
    # åˆ›å»ºæ™ºèƒ½ä½“è¯·æ±‚
    create_agent_request = CreateAgentRequest(
        name="å¸‚åœºåˆ†æå¸ˆ",
        description="ä¸“ä¸šçš„å¸‚åœºè¶‹åŠ¿åˆ†ææ™ºèƒ½ä½“",
        type=AgentType.RAG,
        config=agent_config,
        metadata={"department": "finance", "version": "1.0"}
    )
    print("ğŸ¤– åˆ›å»ºæ™ºèƒ½ä½“è¯·æ±‚:")
    print(json.dumps(create_agent_request.dict(), indent=2, ensure_ascii=False, default=str))
    
    # æ™ºèƒ½ä½“å®ä¾‹å“åº”
    agent_instance_response = AgentInstanceResponse(
        instance_id="inst_001",
        agent_id="agent_001",
        status="active",
        created_at=datetime.now(),
        config={"runtime_config": "optimized"},
        metadata={"performance": "high"}
    )
    print("\nğŸ¤– æ™ºèƒ½ä½“å®ä¾‹å“åº”:")
    print(json.dumps(agent_instance_response.dict(), indent=2, ensure_ascii=False, default=str))


def demo_memory_models():
    """æ¼”ç¤ºè®°å¿†ç®¡ç†æ¨¡å‹çš„ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("4. è®°å¿†ç®¡ç†æ¨¡å‹æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºè®°å¿†è¯·æ±‚
    memory_create_request = MemoryCreateRequest(
        content="ç”¨æˆ·åå¥½ä½¿ç”¨è¯¦ç»†çš„æ•°æ®åˆ†ææŠ¥å‘Šï¼Œå…³æ³¨é•¿æœŸè¶‹åŠ¿",
        type=MemoryType.LONG_TERM,
        importance=MemoryImportance.HIGH,
        agent_id="agent_001",
        thread_id="thread_001",
        tags=["ç”¨æˆ·åå¥½", "åˆ†æé£æ ¼", "é•¿æœŸè®°å¿†"],
        metadata={"category": "user_preference", "confidence": 0.9}
    )
    print("ğŸ§  åˆ›å»ºè®°å¿†è¯·æ±‚:")
    print(json.dumps(memory_create_request.dict(), indent=2, ensure_ascii=False, default=str))
    
    # è®°å¿†é¡¹ç¤ºä¾‹
    memory_item = MemoryItem(
        id="mem_001",
        content="ç”¨æˆ·åœ¨2024å¹´1æœˆè¯¢é—®è¿‡å¸‚åœºè¶‹åŠ¿ï¼Œå¯¹æŠ€æœ¯åˆ†æå¾ˆæ„Ÿå…´è¶£",
        type=MemoryType.EPISODIC,
        importance=MemoryImportance.MEDIUM,
        status=MemoryStatus.ACTIVE,
        agent_id="agent_001",
        thread_id="thread_001",
        created_at=datetime.now(),
        updated_at=datetime.now(),
        access_count=5,
        tags=["å¸‚åœºè¶‹åŠ¿", "æŠ€æœ¯åˆ†æ", "ç”¨æˆ·å…´è¶£"],
        metadata={"session": "2024-01", "topic": "market_analysis"},
        related_memories=["mem_002", "mem_003"]
    )
    print("\nğŸ§  è®°å¿†é¡¹:")
    print(json.dumps(memory_item.dict(), indent=2, ensure_ascii=False, default=str))
    
    # è®°å¿†æœç´¢è¯·æ±‚
    memory_search_request = MemorySearchRequest(
        query="ç”¨æˆ·å¯¹å¸‚åœºåˆ†æçš„åå¥½",
        agent_id="agent_001",
        memory_types=[MemoryType.LONG_TERM, MemoryType.EPISODIC],
        importance_levels=[MemoryImportance.HIGH, MemoryImportance.MEDIUM],
        tags=["ç”¨æˆ·åå¥½", "å¸‚åœºåˆ†æ"],
        limit=10,
        similarity_threshold=0.8
    )
    print("\nğŸ” è®°å¿†æœç´¢è¯·æ±‚:")
    print(json.dumps(memory_search_request.dict(), indent=2, ensure_ascii=False, default=str))


def demo_rag_models():
    """æ¼”ç¤ºRAGç³»ç»Ÿæ¨¡å‹çš„ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("5. RAGç³»ç»Ÿæ¨¡å‹æ¼”ç¤º")
    print("=" * 60)
    
    # æ–‡æ¡£ä¸Šä¼ è¯·æ±‚
    document_upload_request = DocumentUploadRequest(
        title="2024å¹´å¸‚åœºè¶‹åŠ¿æŠ¥å‘Š",
        content="æœ¬æŠ¥å‘Šåˆ†æäº†2024å¹´å…¨çƒå¸‚åœºçš„ä¸»è¦è¶‹åŠ¿...",
        type=DocumentType.PDF,
        source="market_research_team",
        tags=["å¸‚åœºè¶‹åŠ¿", "2024", "åˆ†ææŠ¥å‘Š"],
        metadata={"author": "åˆ†æå¸ˆå›¢é˜Ÿ", "department": "research"},
        chunking_config={"strategy": "semantic", "chunk_size": 1000},
        embedding_config={"model": "openai_3_large"}
    )
    print("ğŸ“„ æ–‡æ¡£ä¸Šä¼ è¯·æ±‚:")
    print(json.dumps(document_upload_request.dict(), indent=2, ensure_ascii=False, default=str))
    
    # æ–‡æ¡£æ¨¡å‹ç¤ºä¾‹
    document_model = DocumentModel(
        id="doc_001",
        title="2024å¹´å¸‚åœºè¶‹åŠ¿æŠ¥å‘Š",
        content="è¯¦ç»†çš„å¸‚åœºåˆ†æå†…å®¹...",
        type=DocumentType.PDF,
        status=DocumentStatus.INDEXED,
        source="market_research_team",
        file_size=2048000,
        created_at=datetime.now(),
        updated_at=datetime.now(),
        indexed_at=datetime.now(),
        chunk_count=50,
        tags=["å¸‚åœºè¶‹åŠ¿", "2024", "åˆ†ææŠ¥å‘Š"],
        metadata={"version": "1.0", "language": "zh-CN"}
    )
    print("\nğŸ“„ æ–‡æ¡£æ¨¡å‹:")
    print(json.dumps(document_model.dict(), indent=2, ensure_ascii=False, default=str))
    
    # RAGè¯·æ±‚ç¤ºä¾‹
    from models.rag_models import RetrievalConfig, EmbeddingModel, VectorStore, ChunkingStrategy
    
    retrieval_config = RetrievalConfig(
        embedding_model=EmbeddingModel.OPENAI_3_LARGE,
        vector_store=VectorStore.CHROMA,
        chunking_strategy=ChunkingStrategy.SEMANTIC,
        chunk_size=1000,
        chunk_overlap=200,
        top_k=5,
        similarity_threshold=0.8,
        rerank=True,
        rerank_model="bge-reranker"
    )
    
    rag_request = RAGRequest(
        query="2024å¹´å¸‚åœºçš„ä¸»è¦è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        collection_name="market_reports",
        retrieval_config=retrieval_config,
        generation_config={"temperature": 0.3, "max_tokens": 1000},
        context_window=4000,
        include_sources=True
    )
    print("\nğŸ” RAGè¯·æ±‚:")
    print(json.dumps(rag_request.dict(), indent=2, ensure_ascii=False, default=str))


def demo_data_validation():
    """æ¼”ç¤ºæ•°æ®éªŒè¯åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("6. æ•°æ®éªŒè¯æ¼”ç¤º")
    print("=" * 60)
    
    # æ­£ç¡®çš„æ•°æ®éªŒè¯
    try:
        valid_chat_request = ChatRequest(
            message="æµ‹è¯•æ¶ˆæ¯",
            agent_id="agent_001",
            temperature=0.5,  # æœ‰æ•ˆèŒƒå›´å†…
            max_tokens=1000   # æœ‰æ•ˆå€¼
        )
        print("âœ… æ•°æ®éªŒè¯æˆåŠŸ:")
        print(f"   æ¶ˆæ¯: {valid_chat_request.message}")
        print(f"   æ¸©åº¦: {valid_chat_request.temperature}")
        print(f"   æœ€å¤§tokens: {valid_chat_request.max_tokens}")
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
    
    # é”™è¯¯çš„æ•°æ®éªŒè¯
    try:
        invalid_chat_request = ChatRequest(
            message="æµ‹è¯•æ¶ˆæ¯",
            agent_id="agent_001",
            temperature=3.0,  # è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (0.0-2.0)
            max_tokens=0      # æ— æ•ˆå€¼ (å¿…é¡» >= 1)
        )
    except Exception as e:
        print(f"\nâŒ é¢„æœŸçš„éªŒè¯é”™è¯¯: {e}")
    
    # åˆ†é¡µå‚æ•°éªŒè¯
    try:
        from models.memory_models import MemoryListRequest
        valid_list_request = MemoryListRequest(
            agent_id="agent_001",
            page=1,
            page_size=20
        )
        print(f"\nâœ… åˆ†é¡µéªŒè¯æˆåŠŸ: é¡µç ={valid_list_request.page}, é¡µå¤§å°={valid_list_request.page_size}")
    except Exception as e:
        print(f"âŒ åˆ†é¡µéªŒè¯å¤±è´¥: {e}")


def demo_api_usage():
    """æ¼”ç¤ºåœ¨APIä¸­çš„å®é™…ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("7. APIä½¿ç”¨æ¼”ç¤º")
    print("=" * 60)
    
    print("ğŸ“‹ åœ¨FastAPIä¸­çš„ä½¿ç”¨ç¤ºä¾‹:")
    print("""
# åœ¨APIè·¯ç”±ä¸­ä½¿ç”¨æ•°æ®æ¨¡å‹

from fastapi import APIRouter, HTTPException
from models.chat_models import ChatRequest, ChatResponse
from models.base_models import BaseResponse

router = APIRouter()

@router.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    '''èŠå¤©APIç«¯ç‚¹'''
    try:
        # è‡ªåŠ¨éªŒè¯è¯·æ±‚æ•°æ®
        # request.message, request.agent_id ç­‰å­—æ®µå·²éªŒè¯
        
        # å¤„ç†èŠå¤©é€»è¾‘
        response_content = await process_chat(request)
        
        # è¿”å›æ ‡å‡†åŒ–å“åº”
        return ChatResponse(
            message=response_content,
            thread_id=request.thread_id or "default",
            agent_id=request.agent_id
        )
    except Exception as e:
        # è¿”å›é”™è¯¯å“åº”
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/health", response_model=BaseResponse)
async def health_endpoint():
    '''å¥åº·æ£€æŸ¥ç«¯ç‚¹'''
    return BaseResponse(
        success=True,
        message="ç³»ç»Ÿè¿è¡Œæ­£å¸¸",
        data={"status": "healthy", "timestamp": datetime.now()}
    )
    """)
    
    print("\nğŸ”„ æ•°æ®åºåˆ—åŒ–å’Œååºåˆ—åŒ–:")
    
    # JSONåºåˆ—åŒ–
    chat_request = ChatRequest(
        message="æµ‹è¯•æ¶ˆæ¯",
        agent_id="agent_001"
    )
    json_data = chat_request.json()
    print(f"åºåˆ—åŒ–ä¸ºJSON: {json_data}")
    
    # JSONååºåˆ—åŒ–
    parsed_request = ChatRequest.parse_raw(json_data)
    print(f"ä»JSONè§£æ: {parsed_request.message}")
    
    # å­—å…¸è½¬æ¢
    dict_data = chat_request.dict()
    print(f"è½¬æ¢ä¸ºå­—å…¸: {dict_data}")
    
    # ä»å­—å…¸åˆ›å»º
    new_request = ChatRequest(**dict_data)
    print(f"ä»å­—å…¸åˆ›å»º: {new_request.agent_id}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Modelsç›®å½•ä½¿ç”¨æ¼”ç¤º")
    print("æœ¬æ¼”ç¤ºå±•ç¤ºäº† /models ç›®å½•ä¸­å„ç§æ•°æ®æ¨¡å‹çš„ä½¿ç”¨æ–¹æ³•")
    
    # è¿è¡Œå„ä¸ªæ¼”ç¤º
    demo_base_models()
    demo_chat_models()
    demo_agent_models()
    demo_memory_models()
    demo_rag_models()
    demo_data_validation()
    demo_api_usage()
    
    print("\n" + "=" * 60)
    print("âœ¨ æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)
    print("""
ğŸ“š Modelsç›®å½•ä½¿ç”¨æ€»ç»“:

1. ğŸ“ ç›®å½•ç»“æ„:
   - base_models.py    # åŸºç¡€å“åº”æ¨¡å‹
   - chat_models.py    # èŠå¤©ç›¸å…³æ¨¡å‹
   - agent_models.py   # æ™ºèƒ½ä½“æ¨¡å‹
   - memory_models.py  # è®°å¿†ç®¡ç†æ¨¡å‹
   - rag_models.py     # RAGç³»ç»Ÿæ¨¡å‹

2. ğŸ¯ ä¸»è¦åŠŸèƒ½:
   - æ•°æ®éªŒè¯å’Œç±»å‹æ£€æŸ¥
   - è‡ªåŠ¨åºåˆ—åŒ–/ååºåˆ—åŒ–
   - APIè¯·æ±‚/å“åº”æ ‡å‡†åŒ–
   - æ–‡æ¡£ç”Ÿæˆæ”¯æŒ

3. ğŸ’¡ ä½¿ç”¨å»ºè®®:
   - åœ¨APIç«¯ç‚¹ä¸­ä½¿ç”¨ä½œä¸ºè¯·æ±‚/å“åº”æ¨¡å‹
   - åˆ©ç”¨Pydanticçš„éªŒè¯åŠŸèƒ½ç¡®ä¿æ•°æ®è´¨é‡
   - ä½¿ç”¨æšä¸¾ç±»å‹æé«˜ä»£ç å¯è¯»æ€§
   - åˆç†è®¾ç½®å­—æ®µçº¦æŸå’Œé»˜è®¤å€¼

4. ğŸ”§ æ‰©å±•æ–¹å¼:
   - ç»§æ‰¿åŸºç¡€æ¨¡å‹ç±»
   - æ·»åŠ è‡ªå®šä¹‰éªŒè¯å™¨
   - ä½¿ç”¨Field()è®¾ç½®å­—æ®µå±æ€§
   - å®šä¹‰æ¨¡å‹é—´çš„å…³ç³»
    """)


if __name__ == "__main__":
    main()